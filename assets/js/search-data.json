{
  
    
        "post0": {
            "title": "A First Shot at Speech Recognition for Khoekhoe",
            "content": "Welcome to my first attempt at a speech recognition algorithm for Khoekhoe! . Khoekhoe (language code: naq) is a Khoisan language primarily spoken in Namibia, but also in some parts of Botswana and South Africa. The language currently has around 280,000 speakers according to Ethnologue and is famous for its employment of clicks and phonemic tone. . As far as I know, there are currently no speech recognition systems in place for Khoekhoe. I&#39;ve personally conducted linguistic research on Khoekhoe (which you can find more about on my website), and I&#39;m now interested in bringing speech recognition (and, in the future, other language technologies) to this amazing language! . The completion of this project will certainly bear social and cultural benefits in Khoekhoe-speaking communities. No longer will speakers be required to use a lingua franca like Afrikaans or English when interacting with speech technology, thereby creating a more technologically accessible and convenient world for these communities. . All of the data that will be used here was collected by me with a single, female Khoekhoe speaker at New York University over the course of the 2017-2018 academic year. . I&#39;ve compiled a .csv file with all of the information we&#39;ll need to try and predict Khoekhoe phones. Let&#39;s take a look at that first: . data = pd.read_csv(&#39;./data/khoekhoe/khoekhoe_utterances.csv&#39;) data.head() . id khoekhoe gloss english phones num_phones wav_path textgrid_path . 0 1 | ǂuru | NaN | healthy | 0,ǂˀ,ˀu,uɾ,ɾu,u,u0,0,0 | 9 | ./voice_recordings/g/g_healthy.wav | ./textgrids/g/g_healthy.TextGrid | . 1 2 | ǂgāp | NaN | bald | 0,0,ǂa,a,ap,0,0p0 | 7 | ./voice_recordings/g/g_bald.wav | ./textgrids/g/g_bald.TextGrid | . 2 3 | ǂhapeb | NaN | sheet of paper | 0,ǂ,ʰ,ʰa,ap,pe,p,0,0,0 | 10 | ./voice_recordings/g/g_paper.wav | ./textgrids/g/g_sheet_of_paper.TextGrid | . 3 4 | ǂkhanis | NaN | book | 0,ǂᵡ,ᵡa,ni,i,is,s | 7 | ./voice_recordings/g/g_book.wav | ./textgrids/g/g_book.TextGrid | . 4 5 | ǂnū | NaN | black | 0,0ᵑ,ᵑǂu,u,u,u0,0 | 7 | ./voice_recordings/g/g_black.wav | ./textgrids/g/g_black.TextGrid | . Details on each column in the dataset: . id: The row&#39;s unique identifier. | khoekhoe: The orthographic representation of the word or phrase in Khoekhoe. | gloss: A literal, word-for-word translation of the sentence. This is only provided when the khoekhoe column has more than one word in it. | english: The natural English translation of the word. | phones: The labels associated with each 100ms interval in the recording found in the file denoted by the wav_path column. | num_phones: The total number of 100ms intervals in the entire recording. | wav_path: Path to the .wav file for the word/phrase in the khoekhoe column. This path assumes that this notebook and the voice_recordings folder are at the root of the working directory. | textgrid_path: Path to the .TextGrid file for the word/phrase in the khoekhoe column. This path assumes that this notebook and the textgrids folder are at the root of the working directory. | . Important information about this data and the way I&#39;ve structured it: . The labels in phones were determined by me using Praat, and is documented by the each .TextGrid file. | Each phone represents the IPA transcription of what is represented in that 100ms interval. &quot;0&quot; represents silence. | . Let&#39;s listen to a few of the sound files, so we have a clear idea of what Khoekhoe sounds like and the data we&#39;re working with. . The following is ǂuru &#39;healthy&#39;. This is found in row 1 of data. . data.loc[0,[&#39;khoekhoe&#39;,&#39;english&#39;,&#39;phones&#39;]] . khoekhoe ǂuru english healthy phones 0,ǂˀ,ˀu,uɾ,ɾu,u,u0,0,0 Name: 0, dtype: object . Your browser does not support the audio element. And a full phrase: Tita ge a !nâ. &#39;I feel hungry.&#39; This is found in row 21 of data. . data.loc[20,[&#39;khoekhoe&#39;,&#39;gloss&#39;,&#39;english&#39;,&#39;phones&#39;]] . khoekhoe Tita ge a !nâ. gloss I DECL COP feel.hungry english I feel hungry. phones 0t,i,ita,ak,ke,eʔ,ʔa,ᵑ,ᵑǃˀ,ã,ã,0 Name: 20, dtype: object . Your browser does not support the audio element. Fantastic! Now that we&#39;re a little more familiar with the data, let&#39;s start to try and recognize phones in Khoekhoe! . We&#39;ll be using convolutional neural networks (CNNs) to predict phones. This means that, for each sound file, we&#39;ll need to do the following: . Load the file in with torchaudio, which gives us an array of numbers. | Split the loaded file into 100ms intervals. | Chop off the last bit/overhang (it won&#39;t be 100ms in length and we want all inputs to be equal size). | Morph our array of numbers into a 1 channel 63x70 &quot;image&quot; (this size was chosen because it&#39;s the closest we could get our original loaded array to a square) so our CNN can read it. | Match each 1x63x70 tensor to its corresponding phone. | Once we do all of this, we&#39;ll need a place to store all of our newly created &quot;images&quot; and their labels, so let&#39;s do that first: . train = pd.DataFrame({&#39;samples_array&#39;: [], &#39;label&#39;: []}) train.head() . samples_array label . Then complete steps 1-5 above! I&#39;ve added some comments in the loop to hopefully make what&#39;s going on clearer. . for row in range(len(data)): # get the sound file path wav_path = data.loc[row, &#39;wav_path&#39;] path = &#39;./data/khoekhoe&#39; + wav_path[1:] # get and split the phones phones_str = data.loc[row, &#39;phones&#39;] phones = phones_str.split(&#39;,&#39;) # load the file samples, sample_rate = torchaudio.load(path) # chop off the overhang n_samples = samples.shape[1] overhang = n_samples % 4410 # sample rate is 44100 samples/sec samples_slice = n_samples - overhang factored_samples = samples[:,:samples_slice] # split the factored_samples array into list of equal-sized arrays # so that each array is equal to 0.1s of the original sound file. # These are each of our 10s segments for the file. sections = int(factored_samples.shape[1] / 4410) tenth_second_intervals = factored_samples.reshape(sections,-1) if len(phones) != tenth_second_intervals.shape[0]: raise ValueError(f&quot;The length of intervals does not match the length of phones in this word/phrase. nWord/Phrase: {data.loc[row, &#39;khoekhoe&#39;]} nIntervals: {len(tenth_second_intervals)} nPhones: {len(phones)}&quot;) for i, interval in enumerate(tenth_second_intervals): # make each interval (i.e., the tensor for each phoneme) # have shape ([1, 63, 70]). # 1 channel of 63x70 &quot;images&quot; of phones. intvl = interval.reshape(63,-1).unsqueeze(0) new_row = {&#39;samples_array&#39;:intvl, &#39;label&#39;:phones[i]} train = train.append(new_row, ignore_index=True) . Now let&#39;s take a look at our training data. . train.head() . samples_array label . 0 [[[tensor(-0.0014), tensor(-0.0013), tensor(-0.0016), tensor(-0.0016), tensor(-0.0018), tensor(-0.0015), tensor(-0.0017), tensor(-0.0017), tensor(-0.0019), tensor(-0.0019), tensor(-0.0018), tensor(-0.0020), tensor(-0.0021), tensor(-0.0020), tensor(-0.0023), tensor(-0.0019), tensor(-0.0019), tensor(-0.0017), tensor(-0.0020), tensor(-0.0019), tensor(-0.0020), tensor(-0.0022), tensor(-0.0018), tensor(-0.0016), tensor(-0.0018), tensor(-0.0019), tensor(-0.0018), tensor(-0.0016), tensor(-0.0017), tensor(-0.0018), tensor(-0.0017), tensor(-0.0017), tensor(-0.0015), tensor(-0.0015), tensor(-0.0015)... | 0 | . 1 [[[tensor(0.0004), tensor(6.1035e-05), tensor(0.), tensor(0.0003), tensor(0.0001), tensor(-6.1035e-05), tensor(-3.0518e-05), tensor(0.0013), tensor(0.0004), tensor(-0.0005), tensor(0.0005), tensor(0.0005), tensor(-0.0002), tensor(-0.0006), tensor(-0.0002), tensor(0.), tensor(-0.0007), tensor(-0.0013), tensor(-0.0009), tensor(-0.0008), tensor(0.0034), tensor(0.0007), tensor(-0.0105), tensor(-0.0004), tensor(0.0143), tensor(-0.0020), tensor(-0.0143), tensor(0.0031), tensor(0.0088), tensor(0.0013), tensor(-0.0077), tensor(-0.0031), tensor(0.0062), tensor(0.0028), tensor(-0.0048), tensor(-0.00... | ǂˀ | . 2 [[[tensor(-0.0066), tensor(-0.0066), tensor(-0.0063), tensor(-0.0060), tensor(-0.0063), tensor(-0.0083), tensor(-0.0095), tensor(-0.0075), tensor(-0.0053), tensor(-0.0045), tensor(-0.0041), tensor(-0.0013), tensor(0.0019), tensor(0.0020), tensor(-0.0018), tensor(-0.0048), tensor(-0.0046), tensor(-0.0031), tensor(-0.0017), tensor(-0.0010), tensor(0.0027), tensor(0.0082), tensor(0.0125), tensor(0.0134), tensor(0.0120), tensor(0.0105), tensor(0.0086), tensor(0.0066), tensor(0.0034), tensor(0.0007), tensor(-0.0012), tensor(-0.0020), tensor(-0.0036), tensor(-0.0062), tensor(-0.0084), tensor(-0.... | ˀu | . 3 [[[tensor(-0.0041), tensor(-0.0031), tensor(-0.0031), tensor(-0.0024), tensor(-0.0013), tensor(-0.0006), tensor(-0.0005), tensor(-0.0004), tensor(-0.0002), tensor(0.0008), tensor(0.0010), tensor(0.0005), tensor(-0.0011), tensor(-0.0020), tensor(-0.0031), tensor(-0.0048), tensor(-0.0051), tensor(-0.0064), tensor(-0.0090), tensor(-0.0107), tensor(-0.0126), tensor(-0.0154), tensor(-0.0173), tensor(-0.0190), tensor(-0.0211), tensor(-0.0235), tensor(-0.0244), tensor(-0.0248), tensor(-0.0267), tensor(-0.0298), tensor(-0.0341), tensor(-0.0360), tensor(-0.0381), tensor(-0.0412), tensor(-0.0434), t... | uɾ | . 4 [[[tensor(0.0047), tensor(0.0029), tensor(6.1035e-05), tensor(-0.0012), tensor(-0.0022), tensor(-0.0028), tensor(-0.0038), tensor(-0.0048), tensor(-0.0062), tensor(-0.0077), tensor(-0.0076), tensor(-0.0072), tensor(-0.0082), tensor(-0.0092), tensor(-0.0097), tensor(-0.0092), tensor(-0.0088), tensor(-0.0086), tensor(-0.0092), tensor(-0.0096), tensor(-0.0091), tensor(-0.0089), tensor(-0.0082), tensor(-0.0079), tensor(-0.0071), tensor(-0.0061), tensor(-0.0045), tensor(-0.0030), tensor(-0.0021), tensor(-0.0014), tensor(-0.0005), tensor(0.0006), tensor(0.0023), tensor(0.0028), tensor(0.0023), t... | ɾu | . Things are looking good! 1 image for 1 label. Let&#39;s check the shape of samples_array in the first row to make sure that we have a 1 channel 63x70 image: . train[&#39;samples_array&#39;][0].shape . torch.Size([1, 63, 70]) . Fantastic! We did it! . Now we can split our newly created data into training and validation sets. Here I use sklearn&#39;s train_test_split function. . train_x, valid_x, train_y, valid_y = train_test_split(train[&#39;samples_array&#39;],train[&#39;label&#39;], test_size=0.2) len(train), train_x.shape, train_y.shape, valid_x.shape, valid_y.shape . (433, (346,), (346,), (87,), (87,)) . Zip these split sets together to tensors and labels together for both the training and validation sets. . training = list(zip(train_x, train_y)) validation = list(zip(valid_x, valid_y)) len(training), len(validation) . (346, 87) . Let&#39;s inspect the beginning of these two newly created lists to make sure things are looking how we want: . training[0][0].shape, training[0][1], validation[0][0].shape, validation[0][1] . (torch.Size([1, 63, 70]), &#39;ap&#39;, torch.Size([1, 63, 70]), &#39;ᵑ&#39;) . training[0] . (tensor([[[ 0.0555, 0.0443, 0.0327, ..., -0.0327, -0.0378, -0.0434], [-0.0483, -0.0522, -0.0562, ..., -0.0934, -0.0906, -0.0806], [-0.0639, -0.0438, -0.0196, ..., 0.0313, 0.0334, 0.0338], ..., [ 0.0086, 0.0084, 0.0075, ..., 0.0111, 0.0136, 0.0161], [ 0.0182, 0.0204, 0.0219, ..., 0.0237, 0.0266, 0.0291], [ 0.0316, 0.0336, 0.0350, ..., -0.0164, -0.0155, -0.0159]]]), &#39;ap&#39;) . Looking good! . To create and train a model, I&#39;ll be using fastai and PyTorch. . Some special formatting is required for us to load the data we&#39;ve just created in training and validation into our model. Fastai&#39;s DataLoader class will allow us to easily do this, while also being able to specify a desired batch_size and any transformations on our data that we&#39;d like to do. . Here, I apply the label_to_index function before the batches are created. This function turns all labels (i.e., phones) into their corresponding index in a sorted list of all of the labels. . We can then use fastai&#39;s DataLoaders class to make our newly created DataLoaders compatible with fastai&#39;s Learner class (which we&#39;ll get to in a moment, after creating our model). . labels = sorted(train[&#39;label&#39;].unique()) def label_to_index(phone): out = [] for tns, lbl in phone: out.append((tns, torch.tensor(labels.index(lbl)))) return out train_dl = DataLoader(training, batch_size=25, before_batch=label_to_index) valid_dl = DataLoader(validation, batch_size=25, before_batch=label_to_index) dls = DataLoaders(train_dl, valid_dl) . Now it&#39;s time to create our model. This is a little complicated, so I won&#39;t go into all of the details here, but see this video for a really comprehensive overview of CNNs. Overall, this model consists of 2 convolutional layers. This is followed by a fully-connected classifier using a log softmax activation function. . I highly encourage anyone trying something like this to uncomment the print() statements in the forward function defined below. It gives the user a comprehensive look at how the network is manipulating its inputs, allowing for much easier debugging. . class Net(nn.Module): def __init__(self, n_input=1, n_output=35, stride=3, n_channel=10): super().__init__() self.conv1 = nn.Conv2d(n_input, n_channel, kernel_size=(9,10), stride=stride) self.bn1 = nn.BatchNorm2d(n_channel) self.pool1 = nn.MaxPool2d((3,2)) # these dimensions are how many rows/cols you want in your pooling layer self.conv2 = nn.Conv2d(n_channel, 2 * n_channel, kernel_size=2) self.bn2 = nn.BatchNorm2d(2 * n_channel) self.pool2 = nn.MaxPool2d(2) self.fc1 = nn.Linear(2 * 80, n_output) def forward(self, x): # print(x.shape) x = self.conv1(x) # print(x.shape) x = F.relu(self.bn1(x)) # print(x.shape) x = self.pool1(x) # print(x.shape) x = self.conv2(x) # print(x.shape) x = F.relu(self.bn2(x)) x = self.pool2(x) # print(x.shape) x = x.view(x.size(0), -1) # print(x.shape) x = self.fc1(x) # print(x.shape) out = F.log_softmax(x, dim=1) # print(out.shape) # print(out) return out model = Net(n_output=len(labels)) . Now we can define our Learner class provided by fastai. For this project, I had to create a new function to determine the accuracy of each batch, which is defined below. . Learner takes in our DataLoaders and CNN model, and provides measurements based on the specified loss_func and metrics. Here we use Cross Entropy Loss as our loss function, and the batch_accuracy function I defined for metrics. . def batch_accuracy(xb, yb): correct = 0 val, idx = torch.max(xb, 1) for i, guess in enumerate(idx): if guess == yb[i]: correct += 1 return correct / 25.0 # divide by batch size learn = Learner(dls, model, loss_func=nn.CrossEntropyLoss(), metrics=batch_accuracy) . Once our Learner is defined, all we need to do is call its .fit() function with however many epochs we&#39;d like to train our model for! You can also specify a learning rate (lr), but it&#39;s not necessary, so we&#39;ll just use fastai&#39;s default (1e-3) for now and then see if we can update it in the future. . learn.fit(20) . epoch train_loss valid_loss batch_accuracy time . 0 | 5.005287 | 4.853674 | 0.240460 | 00:00 | . 1 | 4.552722 | 4.539838 | 0.240460 | 00:00 | . 2 | 4.185816 | 4.295743 | 0.240460 | 00:00 | . 3 | 3.866621 | 4.241688 | 0.251954 | 00:00 | . 4 | 3.594095 | 4.280874 | 0.280460 | 00:00 | . 5 | 3.360926 | 4.428547 | 0.291954 | 00:00 | . 6 | 3.153469 | 4.646707 | 0.291954 | 00:00 | . 7 | 2.961828 | 4.781163 | 0.314943 | 00:00 | . 8 | 2.781793 | 4.885417 | 0.314943 | 00:00 | . 9 | 2.610326 | 4.948970 | 0.314943 | 00:00 | . 10 | 2.447402 | 5.039570 | 0.326437 | 00:00 | . 11 | 2.291741 | 5.107168 | 0.326437 | 00:00 | . 12 | 2.144542 | 5.174448 | 0.337931 | 00:00 | . 13 | 2.005825 | 5.218441 | 0.349425 | 00:00 | . 14 | 1.876026 | 5.302336 | 0.337931 | 00:00 | . 15 | 1.755071 | 5.416248 | 0.349425 | 00:00 | . 16 | 1.642710 | 5.449752 | 0.349425 | 00:00 | . 17 | 1.538720 | 5.512791 | 0.349425 | 00:00 | . 18 | 1.442856 | 5.610217 | 0.349425 | 00:00 | . 19 | 1.354299 | 5.665292 | 0.360920 | 00:00 | . 36.09%. Not bad for a total of 39 data points! But can we make it better? . Fastai&#39;s Learner class comes with a handy .lr_find() function based on Leslie Smith&#39;s learning rate finder. This allows us to see how the loss of our function changes across different magnitudes of learning rates. The ideal learning rate has the steepest slope, because that&#39;s where our loss changes the most. . lr_min,lr_steep = learn.lr_find() . Now that we know which lr has the steepest slope, we can run .fit() again on our Learner, but this time use lr_steep as our learning rate. . learn.fit(20, lr=lr_steep) . epoch train_loss valid_loss batch_accuracy time . 0 | 1.030796 | 5.669055 | 0.349425 | 00:00 | . 1 | 1.030707 | 5.673169 | 0.349425 | 00:00 | . 2 | 1.030594 | 5.673986 | 0.349425 | 00:00 | . 3 | 1.030465 | 5.673920 | 0.349425 | 00:00 | . 4 | 1.030326 | 5.673659 | 0.349425 | 00:00 | . 5 | 1.030177 | 5.673375 | 0.349425 | 00:00 | . 6 | 1.030021 | 5.673069 | 0.349425 | 00:00 | . 7 | 1.029859 | 5.672781 | 0.349425 | 00:00 | . 8 | 1.029692 | 5.672510 | 0.349425 | 00:00 | . 9 | 1.029520 | 5.672239 | 0.349425 | 00:00 | . 10 | 1.029344 | 5.671979 | 0.349425 | 00:00 | . 11 | 1.029166 | 5.671694 | 0.349425 | 00:00 | . 12 | 1.028985 | 5.671468 | 0.349425 | 00:00 | . 13 | 1.028803 | 5.671200 | 0.349425 | 00:00 | . 14 | 1.028619 | 5.670960 | 0.349425 | 00:00 | . 15 | 1.028435 | 5.670724 | 0.349425 | 00:00 | . 16 | 1.028250 | 5.670556 | 0.349425 | 00:00 | . 17 | 1.028066 | 5.670343 | 0.349425 | 00:00 | . 18 | 1.027882 | 5.670082 | 0.349425 | 00:00 | . 19 | 1.027699 | 5.669843 | 0.349425 | 00:00 | . Well look at that, our batch_accuracy has gone down a smidge! Interesting indeed. . Running this a few times, I notice that the batch_accuracy tends to fluctuate between 30 and 35%. . This is so exciting! A first attempt at speech recognition for Khoekhoe has been made! . Let&#39;s take a look now at the predictions we made to see what we&#39;re getting right: . preds,targs = learn.get_preds() argmax_preds = [torch.argmax(i) for i in preds] list(zip(argmax_preds, targs)) . [(tensor(21), tensor(143)), (tensor(64), tensor(54)), (tensor(107), tensor(138)), (tensor(21), tensor(21)), (tensor(0), tensor(0)), (tensor(89), tensor(108)), (tensor(0), tensor(106)), (tensor(0), tensor(0)), (tensor(21), tensor(141)), (tensor(36), tensor(150)), (tensor(97), tensor(41)), (tensor(0), tensor(0)), (tensor(0), tensor(0)), (tensor(0), tensor(131)), (tensor(36), tensor(112)), (tensor(0), tensor(66)), (tensor(49), tensor(77)), (tensor(0), tensor(5)), (tensor(68), tensor(37)), (tensor(0), tensor(0)), (tensor(68), tensor(32)), (tensor(0), tensor(0)), (tensor(77), tensor(140)), (tensor(36), tensor(36)), (tensor(49), tensor(72)), (tensor(0), tensor(0)), (tensor(0), tensor(130)), (tensor(0), tensor(0)), (tensor(0), tensor(0)), (tensor(0), tensor(62)), (tensor(21), tensor(26)), (tensor(37), tensor(31)), (tensor(0), tensor(0)), (tensor(134), tensor(86)), (tensor(0), tensor(0)), (tensor(89), tensor(61)), (tensor(0), tensor(0)), (tensor(21), tensor(78)), (tensor(0), tensor(79)), (tensor(31), tensor(21)), (tensor(49), tensor(49)), (tensor(0), tensor(22)), (tensor(0), tensor(0)), (tensor(31), tensor(49)), (tensor(25), tensor(79)), (tensor(0), tensor(50)), (tensor(36), tensor(118)), (tensor(0), tensor(0)), (tensor(78), tensor(78)), (tensor(36), tensor(78)), (tensor(49), tensor(19)), (tensor(22), tensor(149)), (tensor(0), tensor(65)), (tensor(31), tensor(31)), (tensor(0), tensor(7)), (tensor(89), tensor(111)), (tensor(49), tensor(49)), (tensor(0), tensor(0)), (tensor(107), tensor(71)), (tensor(98), tensor(98)), (tensor(107), tensor(95)), (tensor(0), tensor(0)), (tensor(58), tensor(58)), (tensor(36), tensor(132)), (tensor(36), tensor(21)), (tensor(0), tensor(0)), (tensor(31), tensor(98)), (tensor(0), tensor(22)), (tensor(21), tensor(21)), (tensor(0), tensor(0)), (tensor(0), tensor(0)), (tensor(0), tensor(32)), (tensor(36), tensor(146)), (tensor(22), tensor(52)), (tensor(68), tensor(58)), (tensor(0), tensor(0)), (tensor(0), tensor(115)), (tensor(55), tensor(122)), (tensor(0), tensor(23)), (tensor(21), tensor(21)), (tensor(0), tensor(0)), (tensor(36), tensor(86)), (tensor(0), tensor(102)), (tensor(0), tensor(0)), (tensor(78), tensor(123)), (tensor(0), tensor(0)), (tensor(0), tensor(147))] . The tensors we seem to be predicting correctly some of the time are 0, 21, 31, 36, 49, 58, 78, and 98. So which phones are those? Let&#39;s take a look: . labels[0], labels[21], labels[31], labels[36], labels[49], labels[58], labels[78], labels[98] . (&#39;0&#39;, &#39;a&#39;, &#39;ã&#39;, &#39;e&#39;, &#39;i&#39;, &#39;ĩ&#39;, &#39;o&#39;, &#39;u&#39;) . Vowels and silence! . This makes a lot of sense! Throughout the data there are many silences, so this was to be expected. As for the vowels, well they tend to be longer in length than consonants, and therefore occur more often in the data. . So what are the next steps? Well, I&#39;ll be collecting more data to hopefully be able to pull out some more of those consonants. None of the clicks were detected either, perhaps confirming their difficulty to detect. . Until the next update! .",
            "url": "https://davidbyron.info/blog/speech%20recognition/neural%20networks/computational%20linguistics/khoekhoe/2020/11/21/First-Attempts-at-Speech-Recognition-for-Khoekhoe.html",
            "relUrl": "/speech%20recognition/neural%20networks/computational%20linguistics/khoekhoe/2020/11/21/First-Attempts-at-Speech-Recognition-for-Khoekhoe.html",
            "date": " • Nov 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "A Comparison of TF-IDF, Word2Vec, and Transfer Learning for Text Classification",
            "content": "Text Classification is the assignment of a particular label to a text with respect to its content. In modern Natural Language Processing (NLP), there are many different algorithms and techniques used to gain significant accuracy in text classification tasks. . In this notebook, we will cover three of the most popular methods for text classification: TF-IDF, Word2Vec, and transfer learning. For each of the three methods, we will also show their effectiveness based on the amount of preprocessing that is done to the text beforehand, leaving us with a total of nine measurements at the end. . We will see that transfer learning is by far the superior method for the task in terms of ease of use and accuracy. . The data that we will be using comes from Kaggle&#39;s &quot;Real or Not? NLP with Disaster Tweets&quot; competition, where the user is tasked with predicting which tweets are about real disasters, and which ones are not. . In the competition, leaderboard position is based on the model&#39;s F1 score. Therefore, for clarity, we will provide both the accuracy and F1 score for each output below. . To begin, let&#39;s start with some data analysis and augmentation: . . Tip: You can open this blog post as a notebook in Google Colab using the corresponding badge under the title. This way you can follow along and experiment with the code if you&#8217;d like! . Data Analysis, Augmentation, and Splitting . Light Analysis . import pandas as pd . . First, let&#39;s take a look at the training data we&#39;re given: . data = pd.read_csv(&#39;./data/disaster_tweets/train.csv&#39;) data.head() . id keyword location text target . 0 1 | NaN | NaN | Our Deeds are the Reason of this #earthquake M... | 1 | . 1 4 | NaN | NaN | Forest fire near La Ronge Sask. Canada | 1 | . 2 5 | NaN | NaN | All residents asked to &#39;shelter in place&#39; are ... | 1 | . 3 6 | NaN | NaN | 13,000 people receive #wildfires evacuation or... | 1 | . 4 7 | NaN | NaN | Just got sent this photo from Ruby #Alaska as ... | 1 | . To find out a bit more information about the data, we can use the .info() and .nunique() methods on our DataFrame: . data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 7613 entries, 0 to 7612 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 id 7613 non-null int64 1 keyword 7552 non-null object 2 location 5080 non-null object 3 text 7613 non-null object 4 target 7613 non-null int64 dtypes: int64(2), object(3) memory usage: 297.5+ KB . data.nunique() . id 7613 keyword 221 location 3341 text 7503 target 2 dtype: int64 . Interesting! It looks like some of the tweets (110 of them, to be precise) are the same. . Data Augmentation . Cleaning . import re import spacy . . As mentioned above, I will incorporate different methods of preprocessing to our data to see if such changes have a positive or negative effect on our evaluation metrics. The three differently processed data I&#39;ll be using are: . Unprocessed - the data as it is given to us. | &quot;Simply&quot; cleaned - the data without any hashtags, @-symbols, website links, or punctuation. | SpaCy cleaned - the data lemmatized and without any stop words according to SpaCy&#39;s pretrained English language model (which we&#39;ll get to in a moment). | The unprocessed data is already done for us in the text column of our DataFrame. . Moving on to the second preprocessing method, &quot;simply&quot; cleaned data. By &quot;simply&quot; I mean cleaned explicitly by me using regular expressions with prior assumptions about the data. For the data we&#39;re using here, we have a bunch of tweets. Thererfore, it makes sense to me to remove things like hashtags, @-symbols, and websites, since those don&#39;t intuitively seem like they contribute to a tweets disaster level (though this isn&#39;t necessarily true, just an assumption!). . To achieve this &quot;simple&quot; cleaning of the data, we can use the following three functions I&#39;ve created: . def remove_at_hash(sent): &quot;&quot;&quot; Returns a string with @-symbols and hashtags removed. &quot;&quot;&quot; return re.sub(r&#39;@|#&#39;, r&#39;&#39;, sent.lower()) def remove_sites(sent): &quot;&quot;&quot; Returns a string with any websites starting with &#39;http.&#39; removed. &quot;&quot;&quot; return re.sub(r&#39;http.*&#39;, r&#39;&#39;, sent.lower()) def remove_punct(sent): &quot;&quot;&quot; Returns a string with only English unicode word characters ([a-zA-Z0-9_]). &quot;&quot;&quot; return &#39; &#39;.join(re.findall(r&#39; w+&#39;, sent.lower())) . Now we can create a new column in our data DataFrame that represents the &quot;simply&quot; cleaned tweets. I&#39;ll call this column text_simple. . data[&#39;text_simple&#39;] = data[&#39;text&#39;].apply(lambda x: remove_punct(remove_sites(remove_at_hash(x)))) data.head() . id keyword location text target text_simple . 0 1 | NaN | NaN | Our Deeds are the Reason of this #earthquake M... | 1 | our deeds are the reason of this earthquake ma... | . 1 4 | NaN | NaN | Forest fire near La Ronge Sask. Canada | 1 | forest fire near la ronge sask canada | . 2 5 | NaN | NaN | All residents asked to &#39;shelter in place&#39; are ... | 1 | all residents asked to shelter in place are be... | . 3 6 | NaN | NaN | 13,000 people receive #wildfires evacuation or... | 1 | 13 000 people receive wildfires evacuation ord... | . 4 7 | NaN | NaN | Just got sent this photo from Ruby #Alaska as ... | 1 | just got sent this photo from ruby alaska as s... | . Moving now to the last preprocessing method: SpaCy. SpaCy is a great, open-source software library for NLP. It includes varying, pretrained language models of a number of different sizes for a number of different langauges, allowing you to quickly perform routine NLP tasks. Here, we&#39;re going to use SpaCy to lemmatize each tweet in the data and remove any stop words. . Below, we need to first load in SpaCy&#39;s (full) English model (note that, for speed, I disable some features that we won&#39;t need here). Then, create a function that will give us a string lemmatized by SpaCy. . nlp = spacy.load(&#39;en&#39;, disable=[&#39;ner&#39;, &#39;parser&#39;]) def spacy_cleaning(doc): &quot;&quot;&quot; Returns a string that has been lemmatized and rid of stop words via SpaCy. &quot;&quot;&quot; doc = nlp(doc.lower()) text = [token.lemma_ for token in doc if not token.is_stop] return &#39; &#39;.join(text) . Using our new function, we can again create a new column in our data DataFrame with the SpaCy-cleaned tweets. I&#39;ll call this column text_spacy. . data[&#39;text_spacy&#39;] = data[&#39;text&#39;].apply(lambda x: spacy_cleaning(x)) data.head() . id keyword location text target text_simple text_spacy . 0 1 | NaN | NaN | Our Deeds are the Reason of this #earthquake M... | 1 | our deeds are the reason of this earthquake ma... | deed reason # earthquake allah forgive | . 1 4 | NaN | NaN | Forest fire near La Ronge Sask. Canada | 1 | forest fire near la ronge sask canada | forest fire near la ronge sask . canada | . 2 5 | NaN | NaN | All residents asked to &#39;shelter in place&#39; are ... | 1 | all residents asked to shelter in place are be... | resident ask &#39; shelter place &#39; notify officer ... | . 3 6 | NaN | NaN | 13,000 people receive #wildfires evacuation or... | 1 | 13 000 people receive wildfires evacuation ord... | 13,000 people receive # wildfire evacuation or... | . 4 7 | NaN | NaN | Just got sent this photo from Ruby #Alaska as ... | 1 | just got sent this photo from ruby alaska as s... | get send photo ruby # alaska smoke # wildfire ... | . n-grams . from gensim.models.phrases import Phrases, Phraser . . . Important: I&#8217;ll only be applying what we learn in the n-gram section to the Word2Vec model. If you&#8217;d like to skip this section, and come back when you get to Word2Vec, feel free to do so. . An n-gram is a contiguous sequence of n items from a given sample of text or speech. This turns out to be quite useful in NLP. Consider the phrase &quot;New York Times&quot;. When all three words are together, the phrase is understood to mean the widely spread news source based in New York of the same moniker. However, if we split the words up (while maintaining original order), we get: &quot;New York&quot;, &quot;York Times&quot;, &quot;New&quot;, &quot;York&quot;, and &quot;Times&quot;. These separate words and phrases can occur in many contexts other than those in which the full phrase &quot;New York Times&quot; is found, skewing the phrase&#39;s true meaning in the data. N-gram models allow us to concatenate these commonly occurring multi-word phrases in our data, allowing their true meaning to shine through. . Thankfully, we can use the Phraser and Phrases classes provided by gensim in order to easily find n-grams in our data. . Let&#39;s start by getting trigrams found in the unprocessed data. . First, we extract the tweets and split them by whitespace characters. . text = [re.split(&#39; s+&#39;, tweet) for tweet in data[&#39;text&#39;]] . Then, we find bigrams throughout our data. Here we use a parameter of min_count=30 for our Phrases class. This ensures that only bigrams that occur more than 30 times in the data are found. Many combinations of words occur side by side only a few times, and don&#39;t contribute much additional knowledge to our model, so this is important. . bigram_phrases = Phrases(text, min_count=30) bigram = Phraser(bigram_phrases) bigram_text = bigram[text] . Next, we can use the bigrams we just made to search for trigrams in the exact same way. . Note: This is repeatable! Keep going to find n-grams of size 5 if you wanted! . trigram_phrases = Phrases(bigram_text, min_count=30) trigram = Phraser(trigram_phrases) trigram_text = trigram[bigram_text] . That&#39;s it! Now we can pop this list back into our data DataFrame to be used later. . data[&#39;text_trigram&#39;] = [&#39; &#39;.join(tweet) for tweet in trigram_text] data.head() . id keyword location text target text_simple text_spacy text_trigram . 0 1 | NaN | NaN | Our Deeds are the Reason of this #earthquake M... | 1 | our deeds are the reason of this earthquake ma... | deed reason # earthquake allah forgive | Our Deeds are the Reason of this #earthquake M... | . 1 4 | NaN | NaN | Forest fire near La Ronge Sask. Canada | 1 | forest fire near la ronge sask canada | forest fire near la ronge sask . canada | Forest fire near La Ronge Sask. Canada | . 2 5 | NaN | NaN | All residents asked to &#39;shelter in place&#39; are ... | 1 | all residents asked to shelter in place are be... | resident ask &#39; shelter place &#39; notify officer ... | All residents asked to &#39;shelter in place&#39; are ... | . 3 6 | NaN | NaN | 13,000 people receive #wildfires evacuation or... | 1 | 13 000 people receive wildfires evacuation ord... | 13,000 people receive # wildfire evacuation or... | 13,000 people receive #wildfires evacuation or... | . 4 7 | NaN | NaN | Just got sent this photo from Ruby #Alaska as ... | 1 | just got sent this photo from ruby alaska as s... | get send photo ruby # alaska smoke # wildfire ... | Just got sent this photo from Ruby #Alaska as ... | . Great work! Now let&#39;s do the same for the text_simple and text_spacy columns. . text_simple = [re.split(&#39; s+&#39;, tweet) for tweet in data[&#39;text_simple&#39;]] bigram_phrases = Phrases(text_simple, min_count=30) bigram = Phraser(bigram_phrases) bigram_text_simple = bigram[text_simple] trigram_phrases = Phrases(bigram_text_simple, min_count=30) trigram = Phraser(trigram_phrases) trigram_text_simple = trigram[bigram_text_simple] data[&#39;text_trigram_simple&#39;] = [&#39; &#39;.join(tweet) for tweet in trigram_text_simple] . . text_spacy = [re.split(&#39; s+&#39;, tweet) for tweet in data[&#39;text_spacy&#39;]] bigram_phrases = Phrases(text_spacy, min_count=30) bigram = Phraser(bigram_phrases) bigram_text_spacy = bigram[text_spacy] trigram_phrases = Phrases(bigram_text_spacy, min_count=30) trigram = Phraser(trigram_phrases) trigram_text_spacy = trigram[bigram_text_spacy] data[&#39;text_trigram_spacy&#39;] = [&#39; &#39;.join(tweet) for tweet in trigram_text_spacy] . . data.head() . id keyword location text target text_simple text_spacy text_trigram text_trigram_simple text_trigram_spacy . 0 1 | NaN | NaN | Our Deeds are the Reason of this #earthquake M... | 1 | our deeds are the reason of this earthquake ma... | deed reason # earthquake allah forgive | Our Deeds are the Reason of this #earthquake M... | our deeds are the reason of this earthquake ma... | deed reason # earthquake allah forgive | . 1 4 | NaN | NaN | Forest fire near La Ronge Sask. Canada | 1 | forest fire near la ronge sask canada | forest fire near la ronge sask . canada | Forest fire near La Ronge Sask. Canada | forest fire near la ronge sask canada | forest fire near la ronge sask . canada | . 2 5 | NaN | NaN | All residents asked to &#39;shelter in place&#39; are ... | 1 | all residents asked to shelter in place are be... | resident ask &#39; shelter place &#39; notify officer ... | All residents asked to &#39;shelter in place&#39; are ... | all residents asked to shelter in place are be... | resident ask &#39; shelter place &#39; notify officer ... | . 3 6 | NaN | NaN | 13,000 people receive #wildfires evacuation or... | 1 | 13 000 people receive wildfires evacuation ord... | 13,000 people receive # wildfire evacuation or... | 13,000 people receive #wildfires evacuation or... | 13 000 people receive wildfires evacuation ord... | 13,000 people receive # wildfire evacuation or... | . 4 7 | NaN | NaN | Just got sent this photo from Ruby #Alaska as ... | 1 | just got sent this photo from ruby alaska as s... | get send photo ruby # alaska smoke # wildfire ... | Just got sent this photo from Ruby #Alaska as ... | just got sent this photo from ruby alaska as s... | get send photo ruby # alaska smoke # wildfire ... | . Fantastic! We&#39;ve found all of the trigrams and bigrams in each of our three datasets that occur more than 30 times. This data will prove to be very useful when we reach Word2Vec. . Now that we&#39;ve got the three separately preprocessed sets of tweets in neat columns in our dataset, it&#39;s time to split our data into training and validation data and begin our testing! . Splitting . from sklearn.model_selection import train_test_split . . In order to properly test our data, we&#39;ll need to split it into training and validation sets. To do this, we simply pass our data DataFrame to sklearn&#39;s train_test_split. We reset the index of each newly-created DataFrame to avoid complications with indexing later on. Then, check the shapes to make everything adds up. . train, valid = train_test_split(data, random_state=24) train = train.reset_index() valid = valid.reset_index() train.shape, valid.shape, data.shape . ((5709, 11), (1904, 11), (7613, 10)) . Things are looking good! One last preprocessing step is in order, and that is dividing our newly-created train data by their target labels, thereby giving us two new DataFrames representing disaster tweets and non-disaster tweets. . When we call .nunique() on both disasters and not_disasters, we can see that the unique number of targets in each DataFrame is 1, indicating we split the data properly. . disasters = train[train[&#39;target&#39;] == 1].reset_index() not_disasters = train[train[&#39;target&#39;] == 0].reset_index() disasters.nunique(), not_disasters.nunique() . (level_0 2450 index 2450 id 2450 keyword 220 location 1197 text 2418 target 1 text_simple 2130 text_spacy 2417 text_trigram 2417 text_trigram_simple 2130 text_trigram_spacy 2416 dtype: int64, level_0 3259 index 3259 id 3259 keyword 216 location 1668 text 3239 target 1 text_simple 3069 text_spacy 3237 text_trigram 3239 text_trigram_simple 3069 text_trigram_spacy 3237 dtype: int64) . Awesome! We&#39;re all set and we can begin to train our models. . Let&#39;s start with TF-IDF. . TF-IDF . from collections import defaultdict from gensim.corpora import Dictionary from gensim.models import TfidfModel from gensim.similarities import MatrixSimilarity import numpy as np from sklearn.metrics import accuracy_score, f1_score . . TF-IDF is an incredible, straightforward way to analyze document similarity. It involves no fancy machine learning, just the term frequency across documents! For this reason, we will begin with trying to use TF-IDF to determine if a tweet is about a disaster or not. . From tfidf.com: . Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. . You can learn more about the mathematical foundations of TF-IDF here. We&#39;ll start by analyzing the unprocessed tweets. . TF-IDF with Unprocessed Tweets . In order to calculate the similarity between two tweets (namely, a tweet in the validation set with a tweet in the training set) without having to do all the math out ourselves, we&#39;ll use gensim, a free Python library that provides a lot of great NLP functionality. . Gensim requires a list of texts in a list of documents. For us, that&#39;s a list of words in a tweet in a list of tweets. So let&#39;s make that now. . Note: We&#8217;re using the unprocessed tweets in the text column of our data this time around. We&#8217;ll be using the other two preprocessed tweets in a bit! . disaster_tweets = disasters[&#39;text&#39;].tolist() not_disaster_tweets = not_disasters[&#39;text&#39;].tolist() disaster_tweets_split = [ [word for word in tweet.split()] for tweet in disaster_tweets ] not_disaster_tweets_split = [ [word for word in tweet.split()] for tweet in not_disaster_tweets ] . Thinking about a TF-IDF model, words that only occur once throughout the entire corpus will not provide any noteworthy advantage to the model. Therefore, in the next step, we remove words that only occur once from disaster_tweets_split and not_disaster_tweets split. . disaster_tweets_word_frequency = defaultdict(int) for tweet in disaster_tweets_split: for word in tweet: disaster_tweets_word_frequency[word] += 1 not_disaster_tweets_word_frequency = defaultdict(int) for tweet in not_disaster_tweets_split: for word in tweet: not_disaster_tweets_word_frequency[word] += 1 disaster_tweets_split = [ [word for word in tweet if disaster_tweets_word_frequency[word] &gt; 1] for tweet in disaster_tweets_split ] not_disaster_tweets_split = [ [word for word in tweet if not_disaster_tweets_word_frequency[word] &gt; 1] for tweet in not_disaster_tweets_split ] . Next, we create a Dictionary object with gensim, which is a mapping between words and their integer ids. With this Dictionary object we can create a &quot;corpus&quot; for disaster tweets and non-disaster tweets by converting each document (i.e., tweet) in each set to a Bag of Words format (that is, a list of (token_id, token_count) tuples). . disaster_tweets_dct = Dictionary(disaster_tweets_split) not_disaster_tweets_dct = Dictionary(not_disaster_tweets_split) disaster_tweets_corpus = [disaster_tweets_dct.doc2bow(tweet) for tweet in disaster_tweets_split] not_disaster_tweets_corpus = [not_disaster_tweets_dct.doc2bow(tweet) for tweet in not_disaster_tweets_split] . Fit TF-IDF models for our two sets of tweets. . disaster_tweets_tfidf = TfidfModel(disaster_tweets_corpus) not_disaster_tweets_tfidf = TfidfModel(not_disaster_tweets_corpus) . Apply the models to our corpora to get vectors for each tweet. . disaster_tweets_tfidf_vectors = disaster_tweets_tfidf[disaster_tweets_corpus] not_disaster_tweets_tfidf_vectors = not_disaster_tweets_tfidf[not_disaster_tweets_corpus] . Create variable which we can index into using another vector to compute similarity. . disaster_tweets_similarity = MatrixSimilarity(disaster_tweets_tfidf_vectors) not_disaster_tweets_similarity = MatrixSimilarity(not_disaster_tweets_tfidf_vectors) . Now we can compare each tweet in the validation set to each set of tweets (disaster and non-disaster) in the training set. Whichever set contains a greater number of &quot;similar enough&quot; tweets (to be determined by a threshold) determines how the validation tweet will be labeled. . First, configure the validation tweets in the same way that we did for the training tweets: . valid_tweets = valid[&#39;text&#39;].tolist() valid_tweets_split = [ [word for word in tweet.split()] for tweet in valid_tweets ] valid_tweets_word_frequency = defaultdict(int) for tweet in valid_tweets_split: for word in tweet: valid_tweets_word_frequency[word] += 1 valid_tweets_split = [ [word for word in tweet if valid_tweets_word_frequency[word] &gt; 1] for tweet in valid_tweets_split ] . We now have all the information we need to make our predictions! We can store our predictions in the valid DataFrame. This will make for easier access when comparing target to prediction. . To do that, we need to initialize a new column in the DataFrame, let&#39;s call it prediction: . valid[&#39;prediction&#39;] = np.zeros(len(valid)).astype(&#39;int&#39;) . In order to make predictions using the model we just created, we have to compare each tweet in the validation data with each tweet in both the disasters DataFrame and the not_disasters DataFrame. . Therefore, for each tweet, we: . Turn it into a BoW according to each set of tweets&#39; Dictionary object. | Get a vector for it using each set&#39;s TF-IDF model. | Compare it&#39;s vector with each set&#39;s full set of tweets using the MatrixSimilarity object we created earleir. | Tally up the total number of disaster and non-disaster tweets whose cosine similarity is greater than 0.1. | If the disaster tally is greater than the non-disaster tally, we change the value of the prediction column for this tweet in the valid DataFrame to 1 (otherwise, it stays 0, indicating a non-disastrous guess). | This is exemplefied below: . for row in range(len(valid)): tweet = valid_tweets_split[row] tweet_bow_with_disasters_dct = disaster_tweets_dct.doc2bow(tweet) tweet_bow_with_not_disasters_dct = not_disaster_tweets_dct.doc2bow(tweet) tweet_tfidf_vector_with_disasters_tfidf = disaster_tweets_tfidf[tweet_bow_with_disasters_dct] tweet_tfidf_vector_with_not_disasters_tfidf = not_disaster_tweets_tfidf[tweet_bow_with_not_disasters_dct] disaster_similarity_vector = disaster_tweets_similarity[tweet_tfidf_vector_with_disasters_tfidf] not_disaster_similarity_vector = not_disaster_tweets_similarity[tweet_tfidf_vector_with_not_disasters_tfidf] disaster_tally = np.where(disaster_similarity_vector &gt; 0.1)[0].size # np.where() returns a tuple, so we have to index into [0] to get what we want not_disaster_tally = np.where(not_disaster_similarity_vector &gt; 0.1)[0].size if disaster_tally &gt; not_disaster_tally: valid.loc[row, &#39;prediction&#39;] = 1 . If all went well, we should be able to see our predictions in the valid DataFrame... . valid.head() . index id keyword location text target text_simple text_spacy text_trigram text_trigram_simple text_trigram_spacy prediction . 0 3068 | 4402 | electrocute | NaN | Kids got Disney version of the game Operation ... | 0 | kids got disney version of the game operation ... | kid get disney version game operation 2 aa bat... | Kids got Disney version of the game Operation ... | kids got disney version of the game operation ... | kid get disney version game operation 2 aa bat... | 1 | . 1 3148 | 4522 | emergency | Indianapolis, IN | UPDATE: Indiana State Police reopening I-65 ne... | 1 | update indiana state police reopening i 65 nea... | update : indiana state police reopen i-65 near... | UPDATE: Indiana State Police reopening I-65 ne... | update indiana state police reopening i 65 nea... | update : indiana state police reopen i-65 near... | 1 | . 2 3139 | 4511 | emergency | Phoenix | God forbid anyone in my family knows how to an... | 0 | god forbid anyone in my family knows how to an... | god forbid family know answer phone . need new... | God forbid anyone in my family knows how to an... | god forbid anyone in my family knows how to an... | god forbid family know answer phone . need new... | 0 | . 3 7485 | 10707 | wreck | Alabama, USA | First wreck today. So so glad me and mom are o... | 0 | first wreck today so so glad me and mom are ok... | wreck today . glad mom okay . lot bad . happy ... | First wreck today. So so glad me and mom are o... | first wreck today so so glad me and mom are ok... | wreck today . glad mom okay . lot bad . happy ... | 0 | . 4 6023 | 8608 | seismic | Somalia | Exploration takes seismic shift in Gabon to So... | 0 | exploration takes seismic shift in gabon to so... | exploration take seismic shift gabon somalia -... | Exploration takes seismic shift in Gabon to So... | exploration takes seismic shift in gabon to so... | exploration take seismic shift gabon somalia -... | 0 | . Look at that! Seems we&#39;ve made some predictions! But how well did we do? . Let&#39;s take a look at both the accuracy and F1 score: . accuracy = accuracy_score(valid[&#39;target&#39;], valid[&#39;prediction&#39;]) F1 = f1_score(valid[&#39;target&#39;], valid[&#39;prediction&#39;]) accuracy, F1 . (0.6407563025210085, 0.5302197802197802) . 64.08% accuracy! That&#39;s not too shabby for just looking at word frequencies... . But what happens if we calculate tweet similarities using TF-IDF again, but this time using the preprocessed data that we prepared in the last section? . Let&#39;s start by seeing how our scores improve with the &quot;simply&quot; cleaned tweets. . TF-IDF with &quot;Simple&quot; Tweets . Before we go any further, we&#39;ll need to get rid of the predictions we just made in valid. . valid = valid.drop(columns=[&#39;prediction&#39;]) valid.head() . index id keyword location text target text_simple text_spacy text_trigram text_trigram_simple text_trigram_spacy . 0 3068 | 4402 | electrocute | NaN | Kids got Disney version of the game Operation ... | 0 | kids got disney version of the game operation ... | kid get disney version game operation 2 aa bat... | Kids got Disney version of the game Operation ... | kids got disney version of the game operation ... | kid get disney version game operation 2 aa bat... | . 1 3148 | 4522 | emergency | Indianapolis, IN | UPDATE: Indiana State Police reopening I-65 ne... | 1 | update indiana state police reopening i 65 nea... | update : indiana state police reopen i-65 near... | UPDATE: Indiana State Police reopening I-65 ne... | update indiana state police reopening i 65 nea... | update : indiana state police reopen i-65 near... | . 2 3139 | 4511 | emergency | Phoenix | God forbid anyone in my family knows how to an... | 0 | god forbid anyone in my family knows how to an... | god forbid family know answer phone . need new... | God forbid anyone in my family knows how to an... | god forbid anyone in my family knows how to an... | god forbid family know answer phone . need new... | . 3 7485 | 10707 | wreck | Alabama, USA | First wreck today. So so glad me and mom are o... | 0 | first wreck today so so glad me and mom are ok... | wreck today . glad mom okay . lot bad . happy ... | First wreck today. So so glad me and mom are o... | first wreck today so so glad me and mom are ok... | wreck today . glad mom okay . lot bad . happy ... | . 4 6023 | 8608 | seismic | Somalia | Exploration takes seismic shift in Gabon to So... | 0 | exploration takes seismic shift in gabon to so... | exploration take seismic shift gabon somalia -... | Exploration takes seismic shift in Gabon to So... | exploration takes seismic shift in gabon to so... | exploration take seismic shift gabon somalia -... | . The process this time around will, in fact, be exactly the same as last time! The only change we need to make is that we are indexing into the text_simple column in the disaster_tweets and not_disaster_tweets DataFrames. . Since the procedure is the same, let&#39;s skip to the metrics! (You can still expand the code below if you need a closer look.) . disaster_tweets = disasters[&#39;text_simple&#39;].tolist() not_disaster_tweets = not_disasters[&#39;text_simple&#39;].tolist() disaster_tweets_split = [ [word for word in tweet.split()] for tweet in disaster_tweets ] not_disaster_tweets_split = [ [word for word in tweet.split()] for tweet in not_disaster_tweets ] disaster_tweets_word_frequency = defaultdict(int) for tweet in disaster_tweets_split: for word in tweet: disaster_tweets_word_frequency[word] += 1 not_disaster_tweets_word_frequency = defaultdict(int) for tweet in not_disaster_tweets_split: for word in tweet: not_disaster_tweets_word_frequency[word] += 1 disaster_tweets_split = [ [word for word in tweet if disaster_tweets_word_frequency[word] &gt; 1] for tweet in disaster_tweets_split ] not_disaster_tweets_split = [ [word for word in tweet if not_disaster_tweets_word_frequency[word] &gt; 1] for tweet in not_disaster_tweets_split ] disaster_tweets_dct = Dictionary(disaster_tweets_split) not_disaster_tweets_dct = Dictionary(not_disaster_tweets_split) disaster_tweets_corpus = [disaster_tweets_dct.doc2bow(tweet) for tweet in disaster_tweets_split] not_disaster_tweets_corpus = [not_disaster_tweets_dct.doc2bow(tweet) for tweet in not_disaster_tweets_split] disaster_tweets_tfidf = TfidfModel(disaster_tweets_corpus) not_disaster_tweets_tfidf = TfidfModel(not_disaster_tweets_corpus) disaster_tweets_tfidf_vectors = disaster_tweets_tfidf[disaster_tweets_corpus] not_disaster_tweets_tfidf_vectors = not_disaster_tweets_tfidf[not_disaster_tweets_corpus] disaster_tweets_similarity = MatrixSimilarity(disaster_tweets_tfidf_vectors) not_disaster_tweets_similarity = MatrixSimilarity(not_disaster_tweets_tfidf_vectors) valid_tweets = valid[&#39;text_simple&#39;].tolist() valid_tweets_split = [ [word for word in tweet.split()] for tweet in valid_tweets ] valid_tweets_word_frequency = defaultdict(int) for tweet in valid_tweets_split: for word in tweet: valid_tweets_word_frequency[word] += 1 valid_tweets_split = [ [word for word in tweet if valid_tweets_word_frequency[word] &gt; 1] for tweet in valid_tweets_split ] valid[&#39;prediction&#39;] = np.zeros(len(valid)).astype(&#39;int&#39;) for row in range(len(valid)): tweet = valid_tweets_split[row] tweet_bow_with_disasters_dct = disaster_tweets_dct.doc2bow(tweet) tweet_bow_with_not_disasters_dct = not_disaster_tweets_dct.doc2bow(tweet) tweet_tfidf_vector_with_disasters_tfidf = disaster_tweets_tfidf[tweet_bow_with_disasters_dct] tweet_tfidf_vector_with_not_disasters_tfidf = not_disaster_tweets_tfidf[tweet_bow_with_not_disasters_dct] disaster_similarity_vector = disaster_tweets_similarity[tweet_tfidf_vector_with_disasters_tfidf] not_disaster_similarity_vector = not_disaster_tweets_similarity[tweet_tfidf_vector_with_not_disasters_tfidf] disaster_tally = np.where(disaster_similarity_vector &gt; 0.1)[0].size # np.where() returns a tuple, so we have to index into [0] to get what we want not_disaster_tally = np.where(not_disaster_similarity_vector &gt; 0.1)[0].size if disaster_tally &gt; not_disaster_tally: valid.loc[row, &#39;prediction&#39;] = 1 . . accuracy = accuracy_score(valid[&#39;target&#39;], valid[&#39;prediction&#39;]) F1 = f1_score(valid[&#39;target&#39;], valid[&#39;prediction&#39;]) accuracy, F1 . (0.6659663865546218, 0.5702702702702702) . 66.60% accuracy; we&#39;ve gotten better! Notice that our F1 score has gone up also, from 0.53 to 0.57. . For the last of the TF-IDF similarities, let&#39;s see how things go if we use the tweets that were preprocessed with SpaCy: . TF-IDF with SpaCy Tweets . Same process as before, let&#39;s clear the old predictions from valid and skip to the metrics! . valid = valid.drop(columns=[&#39;prediction&#39;]) . disaster_tweets = disasters[&#39;text_spacy&#39;].tolist() not_disaster_tweets = not_disasters[&#39;text_spacy&#39;].tolist() disaster_tweets_split = [ [word for word in tweet.split()] for tweet in disaster_tweets ] not_disaster_tweets_split = [ [word for word in tweet.split()] for tweet in not_disaster_tweets ] disaster_tweets_word_frequency = defaultdict(int) for tweet in disaster_tweets_split: for word in tweet: disaster_tweets_word_frequency[word] += 1 not_disaster_tweets_word_frequency = defaultdict(int) for tweet in not_disaster_tweets_split: for word in tweet: not_disaster_tweets_word_frequency[word] += 1 disaster_tweets_split = [ [word for word in tweet if disaster_tweets_word_frequency[word] &gt; 1] for tweet in disaster_tweets_split ] not_disaster_tweets_split = [ [word for word in tweet if not_disaster_tweets_word_frequency[word] &gt; 1] for tweet in not_disaster_tweets_split ] disaster_tweets_dct = Dictionary(disaster_tweets_split) not_disaster_tweets_dct = Dictionary(not_disaster_tweets_split) disaster_tweets_corpus = [disaster_tweets_dct.doc2bow(tweet) for tweet in disaster_tweets_split] not_disaster_tweets_corpus = [not_disaster_tweets_dct.doc2bow(tweet) for tweet in not_disaster_tweets_split] disaster_tweets_tfidf = TfidfModel(disaster_tweets_corpus) not_disaster_tweets_tfidf = TfidfModel(not_disaster_tweets_corpus) disaster_tweets_tfidf_vectors = disaster_tweets_tfidf[disaster_tweets_corpus] not_disaster_tweets_tfidf_vectors = not_disaster_tweets_tfidf[not_disaster_tweets_corpus] disaster_tweets_similarity = MatrixSimilarity(disaster_tweets_tfidf_vectors) not_disaster_tweets_similarity = MatrixSimilarity(not_disaster_tweets_tfidf_vectors) valid_tweets = valid[&#39;text_spacy&#39;].tolist() valid_tweets_split = [ [word for word in tweet.split()] for tweet in valid_tweets ] valid_tweets_word_frequency = defaultdict(int) for tweet in valid_tweets_split: for word in tweet: valid_tweets_word_frequency[word] += 1 valid_tweets_split = [ [word for word in tweet if valid_tweets_word_frequency[word] &gt; 1] for tweet in valid_tweets_split ] valid[&#39;prediction&#39;] = np.zeros(len(valid)).astype(&#39;int&#39;) for row in range(len(valid)): tweet = valid_tweets_split[row] tweet_bow_with_disasters_dct = disaster_tweets_dct.doc2bow(tweet) tweet_bow_with_not_disasters_dct = not_disaster_tweets_dct.doc2bow(tweet) tweet_tfidf_vector_with_disasters_tfidf = disaster_tweets_tfidf[tweet_bow_with_disasters_dct] tweet_tfidf_vector_with_not_disasters_tfidf = not_disaster_tweets_tfidf[tweet_bow_with_not_disasters_dct] disaster_similarity_vector = disaster_tweets_similarity[tweet_tfidf_vector_with_disasters_tfidf] not_disaster_similarity_vector = not_disaster_tweets_similarity[tweet_tfidf_vector_with_not_disasters_tfidf] disaster_tally = np.where(disaster_similarity_vector &gt; 0.1)[0].size # np.where() returns a tuple, so we have to index into [0] to get what we want not_disaster_tally = np.where(not_disaster_similarity_vector &gt; 0.1)[0].size if disaster_tally &gt; not_disaster_tally: valid.loc[row, &#39;prediction&#39;] = 1 . . accuracy = accuracy_score(valid[&#39;target&#39;], valid[&#39;prediction&#39;]) F1 = f1_score(valid[&#39;target&#39;], valid[&#39;prediction&#39;]) accuracy, F1 . (0.6402310924369747, 0.4891871737509322) . With SpaCy lemmatization and removal of stop words, we&#39;ve actually gotten the worst results of the three datasets, with an accuracy of 64.02% and an F1 score of 0.49. . So it seems of the three preprocessing techniques used in a TF-IDF model, in this case, &quot;simple&quot; cleaning worked the best with an accuracy of 66.60 and an F1 score of 0.57. . Let&#39;s now move forward with Word2Vec. . Word2Vec . from gensim.models import Word2Vec from gensim.models.phrases import Phrases, Phraser import time . . The second method for text classification that we&#39;ll use is word vectors. . Word vectors were first introduced by Mikolov et al.[1][2] and provide highly accurate results in word similarity tasks at relatively low computational cost. You can think of a word vector as a 1-dimensional matrix of numbers of some arbitrary length computed by neural networks. Word similarity is then determined by the cosine distance between two vectors. . Word vectors, interestingly, can encode linguistic regularities and patterns. Therefore, many of these patterns can be represented as linear translations. For example vector(king) - vector(man) + vector(woman) is going to very close to vector(queen). This is surprising! . Let&#39;s see how word vectors do at predicting disaster tweets. . Word2Vec Preprocessing . We&#39;ll be using gensim&#39;s Word2Vec module, which processes text using a min_count parameter. This parameter only includes words in the input that occur more than the set min_count number of times. This will cause problems later on when trying to classify the tweets in the validation set because some of the words will have occurred less than the min_count parameter, throwing an &quot;out-of-vocabulary&quot; (OOV) error. . In order to remedy this, we have two options: . Train the Word2Vec model and then remove the words from the validation tweets that are not in the trained vocabulary. | Preemptively change the words in our corpus that occur less than the expected min_count number of times with some sort of &quot;unknown&quot; character. | Both of these methods alter the original tweet that we&#39;ll be classifying, but the latter option seems to adhere closer to the original meaning of the tweet. If we drop words, we could make an entirely new sentence with an enitrely new grammatical structure and meaning. Whereas if we replace the words that occur less than min_count amount of times with an unknown character, the original grammatical structure of each sentence is held in tact, creating a closer tie to the tweet&#39;s original meaning. . To do this efficiently, I&#39;ve created a function replace_unknowns() that replaces the words in a text which occur less than a specified min_count number of times with &#39;UNK&#39;. We can use this to alter the preprocessed columns that we made earlier and store them in our original data DataFrame. . def replace_unknowns(search_texts, min_count): &quot;&quot;&quot; Replaces words that occur less than a certain number of times in a string or list of strings with &#39;UNK&#39;. Parameters - search_texts : list A list of input strings to iterate over. min_count : int An integer specify the minimum count a word should occur in the search_texts to not be replaced with &#39;UNK&#39;. Returns - list List of search_texts with words that occur less than the min_count amount of times replaced with &#39;UNK&#39;. &quot;&quot;&quot; # Get all tweets lowered and tokenized. # This makes sense because we&#39;d never want to # treat an &#39;a&#39; different from an &#39;A&#39;. # (Capitalization is just an orthographical convention) texts = [ [word for word in re.split(&#39; s+&#39;, text.lower())] for text in search_texts ] # create a dictionary that stores the count of each # word in our uncleaned tweets. We can insert new words # into the dict or add to their count if their already in it. vocab_counts = defaultdict(int) # Create a list that we can append words that occur more than # the desired threshold number of times to. vocab = [] for text in texts: for word in text: vocab_counts[word] += 1 # Now go through the vocab_counts and get rid of # words that occur less than five times. for word in vocab_counts.keys(): if vocab_counts[word] &gt; min_count: vocab.append(word) # Now initialize a new column in data that will hold # the tweets with &#39;UNK&#39; replacing words that occur # across the entire vocabulary less than five times. # This creates congruency later on in the model. # data[&#39;text_count_5&#39;] = np.empty(len(data), dtype=str) # ***** DO THIS OUTSIDE FUNC IN WORD2VEC SECTION # Now, go through each tweet and replace the words that # occur less than 5 times throughout the entire corpus # with &#39;UNK&#39;. Then, we insert the new tweet into a new # column in the original dataframe. out = [] # this process takes about a minute for i, text in enumerate(texts): text_replaced = [] for word in text: if word in vocab: text_replaced.append(word) else: text_replaced.append(&#39;UNK&#39;) text_replaced = &#39; &#39;.join(text_replaced) out.append(text_replaced) return out . . Below, we&#39;ll use min_count=5 as one of our hyperparameters) in our Word2Vec model, so let&#39;s replace all of the words in all three of our preprocessed tweet columns (text_trigram, text_trigram_simple, and text_trigram_spacy) in each DataFrame with &#39;UNK&#39;. . Important: We&#8217;re using the tweets with the n-grams that we built in the Data Augmentation section for our Word2Vec model. If you skipped it, go back now! . Note: Normally this would happen during the initial preprocessing stage, allowing us to only need to call replace_unknowns() on our initial data DataFrame. Because we&#8217;re calling replace_unknowns() after we&#8217;ve already split our data into training and validation sets, we need to call the function on all of the DataFrames we&#8217;ve already created. . data[&#39;text_count_5&#39;] = replace_unknowns(data[&#39;text_trigram&#39;], 5) data[&#39;text_simple_5&#39;] = replace_unknowns(data[&#39;text_trigram_simple&#39;], 5) data[&#39;text_spacy_5&#39;] = replace_unknowns(data[&#39;text_trigram_spacy&#39;], 5) data.head() . . id keyword location text target text_simple text_spacy text_trigram text_trigram_simple text_trigram_spacy text_count_5 text_simple_5 text_spacy_5 . 0 1 | NaN | NaN | Our Deeds are the Reason of this #earthquake M... | 1 | our deeds are the reason of this earthquake ma... | deed reason # earthquake allah forgive | Our Deeds are the Reason of this #earthquake M... | our deeds are the reason of this earthquake ma... | deed reason # earthquake allah forgive | our UNK are the reason of this #earthquake may... | our UNK are the reason of this earthquake may ... | UNK reason # earthquake allah UNK | . 1 4 | NaN | NaN | Forest fire near La Ronge Sask. Canada | 1 | forest fire near la ronge sask canada | forest fire near la ronge sask . canada | Forest fire near La Ronge Sask. Canada | forest fire near la ronge sask canada | forest fire near la ronge sask . canada | forest fire near la UNK UNK canada | forest fire near la UNK UNK canada | forest fire near la UNK UNK . canada | . 2 5 | NaN | NaN | All residents asked to &#39;shelter in place&#39; are ... | 1 | all residents asked to shelter in place are be... | resident ask &#39; shelter place &#39; notify officer ... | All residents asked to &#39;shelter in place&#39; are ... | all residents asked to shelter in place are be... | resident ask &#39; shelter place &#39; notify officer ... | all residents asked to UNK in UNK are being UN... | all residents asked to shelter in place are be... | resident ask &#39; shelter place &#39; UNK officer . e... | . 3 6 | NaN | NaN | 13,000 people receive #wildfires evacuation or... | 1 | 13 000 people receive wildfires evacuation ord... | 13,000 people receive # wildfire evacuation or... | 13,000 people receive #wildfires evacuation or... | 13 000 people receive wildfires evacuation ord... | 13,000 people receive # wildfire evacuation or... | UNK people UNK UNK evacuation orders in califo... | 13 UNK people UNK wildfires evacuation orders ... | UNK people UNK # wildfire evacuation order cal... | . 4 7 | NaN | NaN | Just got sent this photo from Ruby #Alaska as ... | 1 | just got sent this photo from ruby alaska as s... | get send photo ruby # alaska smoke # wildfire ... | Just got sent this photo from Ruby #Alaska as ... | just got sent this photo from ruby alaska as s... | get send photo ruby # alaska smoke # wildfire ... | just got sent this photo from UNK UNK as smoke... | just got sent this photo from UNK alaska as sm... | get send photo UNK # alaska smoke # wildfire U... | . valid[&#39;text_count_5&#39;] = replace_unknowns(valid[&#39;text_trigram&#39;], 5) valid[&#39;text_simple_5&#39;] = replace_unknowns(valid[&#39;text_trigram_simple&#39;], 5) valid[&#39;text_spacy_5&#39;] = replace_unknowns(valid[&#39;text_trigram_spacy&#39;], 5) disasters[&#39;text_count_5&#39;] = replace_unknowns(disasters[&#39;text_trigram&#39;], 5) disasters[&#39;text_simple_5&#39;] = replace_unknowns(disasters[&#39;text_trigram_simple&#39;], 5) disasters[&#39;text_spacy_5&#39;] = replace_unknowns(disasters[&#39;text_trigram_spacy&#39;], 5) not_disasters[&#39;text_count_5&#39;] = replace_unknowns(not_disasters[&#39;text_trigram&#39;], 5) not_disasters[&#39;text_simple_5&#39;] = replace_unknowns(not_disasters[&#39;text_trigram_simple&#39;], 5) not_disasters[&#39;text_spacy_5&#39;] = replace_unknowns(not_disasters[&#39;text_trigram_spacy&#39;], 5) . . Great, now our data is set up and ready to be used with a Word2Vec model! . Word2Vec with Unprocessed Tweets . First and foremost, let&#39;s get rid of the valid[&#39;prediction&#39;] column that we made using TF-IDF. . valid = valid.drop(columns=[&#39;prediction&#39;]) . Initialize our Word2Vec model. . Note: I&#8217;m splitting up the training of the model into three steps. See this notebook for more details on why (and Word2Vec in general). . model = Word2Vec(min_count=5, sample=1e-3, workers=4, seed=24) . Build the vocab for our model. . The .build_vocab() method expects an iterable of a list of strings as its input, so first we split our tweets to adhere to that. Notice that we&#39;re looping through all of the tweets in our original data DataFrame rather than the train DataFrame we created. This is because we need the vocabulary of all tweets (in both the training and validation data) in order to properly compare tweets in the training data to tweets in the validation data. If we just built our model on the training data, many of the words in the validation tweets would throw OOV errors! . tweets = [ [wd for wd in tweet.split(&#39; &#39;)] for tweet in data[&#39;text_count_5&#39;] ] model.build_vocab(tweets) . Now we can train the model over 30 epochs (cycles). . model.train(tweets, total_examples=model.corpus_count, epochs=30) . (2006054, 3383040) . Now we normalize vectors in the vocaulary for consistency. . Important: You wouldn&#8217;t do this if you were going to train further down the line. See this notebook for more information about expanding your model&#8217;s vocabulary. . model.wv.init_sims(replace=True) . Now we can make our predictions. . Just as when we were doing TF-IDF, we need to initialize a prediction column in the valid DataFrame to store our predictions. . valid[&#39;prediction&#39;] = np.zeros(len(valid)).astype(&#39;int&#39;) . Similar to how we predicted whether a tweet was a disaster or not with TF-IDF, we have to compare each tweet in the validation data with each tweet in both the disasters DataFrame and the not_disasters DataFrame. . So, this time, for each tweet, we: . Split the validation tweet on all whitespace characters. | Calculate the similarity between the validation tweet and each disaster and non-disaster tweet (also split on whitespace characters). | If the similarity between the two tweets is greater than 0.7, add to that tweet set&#39;s tally. | If the disaster tally is gerater than the non-disaster tally, we change the value of the prediction column for the validation tweet to 1 (otherwise, it remains 0, indicating a non-disastrous guess). | This is exemplified below: . Note: This model takes a little bit of time to train. It took almost 16 minutes on my machine. . start_time = time.time() for valid_row in range(len(valid)): valid_tweet = valid.loc[valid_row, &#39;text_count_5&#39;] tokenized_valid_tweet = re.split(&#39; s+&#39;, valid_tweet) # split on all whitespace characters disaster_count = 0 not_disaster_count = 0 # we can just reuse &quot;disasters&quot; and # &quot;not_disasters&quot; from earlier! for disaster_row in range(len(disasters)): disaster_tweet = disasters.loc[disaster_row, &#39;text_count_5&#39;] tokenized_disaster_tweet = re.split(&#39; s+&#39;, disaster_tweet) if model.wv.n_similarity(tokenized_valid_tweet, tokenized_disaster_tweet) &gt; 0.7: disaster_count += 1 for not_disaster_row in range(len(not_disasters)): not_disaster_tweet = not_disasters.loc[not_disaster_row, &#39;text_count_5&#39;] tokenized_not_disaster_tweet = re.split(&#39; s+&#39;, not_disaster_tweet) if model.wv.n_similarity(tokenized_valid_tweet, tokenized_not_disaster_tweet) &gt; 0.7: not_disaster_count += 1 if disaster_count &gt; not_disaster_count: valid.loc[valid_row, &#39;prediction&#39;] = 1 end_time = time.time() print(f&#39;Runtime: {(end_time - start_time) / 60.0} mins&#39;) . Runtime: 15.00377383629481 mins . Now let&#39;s take another look at the valid DataFrame to see if we&#39;ve got some predictions... . valid.head() . index id keyword location text target text_simple text_spacy text_trigram text_trigram_simple text_trigram_spacy text_count_5 text_simple_5 text_spacy_5 prediction . 0 3068 | 4402 | electrocute | NaN | Kids got Disney version of the game Operation ... | 0 | kids got disney version of the game operation ... | kid get disney version game operation 2 aa bat... | Kids got Disney version of the game Operation ... | kids got disney version of the game operation ... | kid get disney version game operation 2 aa bat... | UNK got UNK UNK of the game UNK only 2 UNK UNK... | kids got UNK UNK of the game UNK only 2 UNK UN... | kid get UNK version game UNK 2 UNK UNK ? UNK o... | 0 | . 1 3148 | 4522 | emergency | Indianapolis, IN | UPDATE: Indiana State Police reopening I-65 ne... | 1 | update indiana state police reopening i 65 nea... | update : indiana state police reopen i-65 near... | UPDATE: Indiana State Police reopening I-65 ne... | update indiana state police reopening i 65 nea... | update : indiana state police reopen i-65 near... | update: UNK state police UNK UNK near UNK UNK ... | update UNK state police UNK i UNK near UNK UNK... | update : UNK state police UNK UNK near UNK UNK... | 0 | . 2 3139 | 4511 | emergency | Phoenix | God forbid anyone in my family knows how to an... | 0 | god forbid anyone in my family knows how to an... | god forbid family know answer phone . need new... | God forbid anyone in my family knows how to an... | god forbid anyone in my family knows how to an... | god forbid family know answer phone . need new... | god UNK UNK in my family UNK how to UNK a UNK ... | god UNK UNK in my family UNK how to UNK a phon... | god UNK family know UNK phone . need new emerg... | 0 | . 3 7485 | 10707 | wreck | Alabama, USA | First wreck today. So so glad me and mom are o... | 0 | first wreck today so so glad me and mom are ok... | wreck today . glad mom okay . lot bad . happy ... | First wreck today. So so glad me and mom are o... | first wreck today so so glad me and mom are ok... | wreck today . glad mom okay . lot bad . happy ... | first wreck UNK so so UNK me and UNK are UNK U... | first wreck today so so UNK me and UNK are UNK... | wreck today . UNK UNK UNK . lot bad . UNK UNK ... | 0 | . 4 6023 | 8608 | seismic | Somalia | Exploration takes seismic shift in Gabon to So... | 0 | exploration takes seismic shift in gabon to so... | exploration take seismic shift gabon somalia -... | Exploration takes seismic shift in Gabon to So... | exploration takes seismic shift in gabon to so... | exploration take seismic shift gabon somalia -... | UNK UNK seismic UNK in UNK to UNK - UNK UNK UN... | UNK UNK seismic UNK in UNK to UNK UNK UNK | UNK take seismic UNK UNK UNK - UNK ( UNK ) UNK... | 0 | . Seems to have worked! . Now let&#39;s find out the accuracy and F1 score of our Word2Vec model using the unprocessed tweet data. . accuracy = accuracy_score(valid[&#39;target&#39;], valid[&#39;prediction&#39;]) F1 = f1_score(valid[&#39;target&#39;], valid[&#39;prediction&#39;]) accuracy, F1 . (0.6339285714285714, 0.277720207253886) . 63.39% accuracy! That&#39;s about the same as the TF-IDF model. The F1 score on the other hand... yikes! 0.28. Horrible! . Note: There&#8217;s a bit of randomness involved when making predictions with Word2Vec and transfer learning (coming up). For this reason, if you run this notebook, your metrics may be a little different than what is shown here. Can we improve that with either of the preprocessed tweets? . Word2Vec with &quot;Simple&quot; Tweets . Once again, we clear out the predictions we&#39;ve just made from valid. . valid = valid.drop(columns=[&#39;prediction&#39;]) . Just like with TF-IDF (seeing a trend here?), the process this time around will be exactly the same as before. The only change we need to make is that we are indexing into the text_simple_5 column in the disaster_tweets and not_disaster_tweets DataFrames. . Since the procedures are the same, let&#39;s skip to the metrics! (You can still expand the code below if you need a closer look.) . start_time = time.time() model = Word2Vec(min_count=5, sample=1e-3, workers=4, seed=24) tweets = [ [wd for wd in tweet.split(&#39; &#39;)] for tweet in data[&#39;text_simple_5&#39;] ] model.build_vocab(tweets) model.train(tweets, total_examples=model.corpus_count, epochs=30) model.wv.init_sims(replace=True) valid[&#39;prediction&#39;] = np.zeros(len(valid)).astype(&#39;int&#39;) for valid_row in range(len(valid)): valid_tweet = valid.loc[valid_row, &#39;text_simple_5&#39;] tokenized_valid_tweet = re.split(&#39; s+&#39;, valid_tweet) # split on all whitespace characters disaster_count = 0 not_disaster_count = 0 # we can just reuse &quot;disasters&quot; and # &quot;not_disasters&quot; from earlier! for disaster_row in range(len(disasters)): disaster_tweet = disasters.loc[disaster_row, &#39;text_simple_5&#39;] tokenized_disaster_tweet = re.split(&#39; s+&#39;, disaster_tweet) if model.wv.n_similarity(tokenized_valid_tweet, tokenized_disaster_tweet) &gt; 0.7: disaster_count += 1 for not_disaster_row in range(len(not_disasters)): not_disaster_tweet = not_disasters.loc[not_disaster_row, &#39;text_simple_5&#39;] tokenized_not_disaster_tweet = re.split(&#39; s+&#39;, not_disaster_tweet) if model.wv.n_similarity(tokenized_valid_tweet, tokenized_not_disaster_tweet) &gt; 0.7: not_disaster_count += 1 if disaster_count &gt; not_disaster_count: valid.loc[valid_row, &#39;prediction&#39;] = 1 end_time = time.time() print(f&#39;Runtime: {(end_time - start_time) / 60.0} mins&#39;) . . Runtime: 15.7726225177447 mins . accuracy = accuracy_score(valid[&#39;target&#39;], valid[&#39;prediction&#39;]) F1 = f1_score(valid[&#39;target&#39;], valid[&#39;prediction&#39;]) accuracy, F1 . (0.6727941176470589, 0.4331210191082802) . Quite an improvement! Our accuracy and F1 score went up to 67.28% and 0.43, respectively. . Now let&#39;s see how the SpaCy tweets perform in our Word2Vec model. . Word2Vec with SpaCy Tweets . Same process as before, let&#39;s clear the old predictions from valid and skip to the metrics! . valid = valid.drop(columns=[&#39;prediction&#39;]) . start_time = time.time() model = Word2Vec(min_count=5, sample=1e-3, workers=4, seed=24) tweets = [ [wd for wd in tweet.split(&#39; &#39;)] for tweet in data[&#39;text_spacy_5&#39;] ] model.build_vocab(tweets) model.train(tweets, total_examples=model.corpus_count, epochs=30) model.wv.init_sims(replace=True) valid[&#39;prediction&#39;] = np.zeros(len(valid)).astype(&#39;int&#39;) for valid_row in range(len(valid)): valid_tweet = valid.loc[valid_row, &#39;text_spacy_5&#39;] tokenized_valid_tweet = re.split(&#39; s+&#39;, valid_tweet) # split on all whitespace characters disaster_count = 0 not_disaster_count = 0 # we can just reuse &quot;disasters&quot; and # &quot;not_disasters&quot; from earlier! for disaster_row in range(len(disasters)): disaster_tweet = disasters.loc[disaster_row, &#39;text_spacy_5&#39;] tokenized_disaster_tweet = re.split(&#39; s+&#39;, disaster_tweet) if model.wv.n_similarity(tokenized_valid_tweet, tokenized_disaster_tweet) &gt; 0.7: disaster_count += 1 for not_disaster_row in range(len(not_disasters)): not_disaster_tweet = not_disasters.loc[not_disaster_row, &#39;text_spacy_5&#39;] tokenized_not_disaster_tweet = re.split(&#39; s+&#39;, not_disaster_tweet) if model.wv.n_similarity(tokenized_valid_tweet, tokenized_not_disaster_tweet) &gt; 0.7: not_disaster_count += 1 if disaster_count &gt; not_disaster_count: valid.loc[valid_row, &#39;prediction&#39;] = 1 end_time = time.time() print(f&#39;Runtime: {(end_time - start_time) / 60.0} mins&#39;) . . Runtime: 13.444611112276712 mins . accuracy = accuracy_score(valid[&#39;target&#39;], valid[&#39;prediction&#39;]) F1 = f1_score(valid[&#39;target&#39;], valid[&#39;prediction&#39;]) accuracy, F1 . (0.6444327731092437, 0.3236763236763237) . SpaCy, this time, comes in the middle of our three tests with an accuracy of 64.44% and F1 score of 0.32. . Among the three datasets trained with a Word2Vec model, the &quot;simple&quot; tweets seem to have it again with an accuracy of 67.28% and an F1 score of 0.43. . Lastly, let&#39;s turn to transfer learning. . Transfer Learning with fastai . from fastai.text.all import * . . Rather than create our own neural network from scratch that competes with something like Word2Vec, we can use transfer learning to quickly adapt our language data by using a model that&#39;s already been trained on a lot more data than just what we have. . From Jason Brownlee: . Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task. . In order to perform transfer learning, we&#39;ll be using fastai. Fastai is great because it really simplifies the training procedure, thereby making it super easy to perform an array of deep learning tasks. We&#39;ll need two classes from fastai to conduct transfer learning with text: language_model_learner and text_classifier_learner. The former will allow us to shape the pretrained model with our own data to make a new language model, while the latter will allow us to create a classifier model for the tweets we have (the same task we&#39;ve been doing above). . Let&#39;s start, per usual, with the unprocessed tweets. . Transfer Learning with Unprocessed Tweets . Fastai uses PyTorch under the hood, which requires our data to be formatted in a specific way. In order to do this most efficiently, we can use fastai&#39;s DataBlock object and .dataloaders() method. With DataBlock, we can: . Directly pull our columns from the dataframe that we&#39;d like to train and test on. | Split the data however we&#39;d like. | And more! | Let&#39;s start by creating a DataBlock that we&#39;ll pass to language_model_learner to create a new language model tailored to our data. . dls_lm = DataBlock( blocks=(TextBlock.from_df(&#39;text&#39;, is_lm=True)), get_items=ColReader(&#39;text&#39;), splitter=RandomSplitter(0.1) ).dataloaders(data, bs=128, seq_len=80) . Note that there is only one block in the DataBlock we just created: a TextBlock. All we need to create a language model is the text (we don&#39;t care about the categories yet), so we only need one block in the DataBlock. We also need to specify the parameter is_lm=True when creating the TextBlock, to specify that this is our language model. . Now we can use .show_batch() to take a look at our newly formatted data: . dls_lm.show_batch(max_n=2) . text text_ . 0 xxbos xxmaj debris found on xxmaj reunion xxmaj island comes from xxup mh370 : xxmaj malaysian xxup pm http : / / t.co / xxunk xxbos xxunk xxmaj what a fucking idiot . xxmaj he had a gun &amp; &amp; a hatchet yet there were still no serious injuries . xxmaj glad police xxunk him . xxbos xxmaj xxunk just died a thousand deaths ! xxrep 4 ? http : / / t.co / xxunk xxbos &#39; your body will | xxmaj debris found on xxmaj reunion xxmaj island comes from xxup mh370 : xxmaj malaysian xxup pm http : / / t.co / xxunk xxbos xxunk xxmaj what a fucking idiot . xxmaj he had a gun &amp; &amp; a hatchet yet there were still no serious injuries . xxmaj glad police xxunk him . xxbos xxmaj xxunk just died a thousand deaths ! xxrep 4 ? http : / / t.co / xxunk xxbos &#39; your body will heal | . 1 indian xxunk xxunk _ http : / / t.co / xxunk xxbos xxmaj the xxunk xxunk and xxunk has sunk ever lower and xxmaj xxunk who has served on the court since 1998 has … http : / / t.co / xxunk xxbos xxunk _ did n&#39;t look like a murder scene just 1 cops a fire truck and 2 fire assistance cars along with a helicopter xxbos xxunk xxmaj there are xxmaj christian terrorists to be sure but i | xxunk xxunk _ http : / / t.co / xxunk xxbos xxmaj the xxunk xxunk and xxunk has sunk ever lower and xxmaj xxunk who has served on the court since 1998 has … http : / / t.co / xxunk xxbos xxunk _ did n&#39;t look like a murder scene just 1 cops a fire truck and 2 fire assistance cars along with a helicopter xxbos xxunk xxmaj there are xxmaj christian terrorists to be sure but i do | . We can now instantiate our language_model_learner using the DataBlock we just created and AWD_LSTM, which is a pretrained model provided by fastai. You can learn more about AWD_LSTM here. . learn = language_model_learner( dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]) . All that&#39;s left to do is fit our language model! . You&#39;ll note that fastai also provides super clear, customizable output for each training cycle. . learn.fit_one_cycle(1, 2e-2) . epoch train_loss valid_loss accuracy perplexity time . 0 | 4.493653 | 3.554004 | 0.404785 | 34.952980 | 01:48 | . learn.fit_one_cycle(10,2e-3) . epoch train_loss valid_loss accuracy perplexity time . 0 | 3.789885 | 3.534729 | 0.407548 | 34.285706 | 01:46 | . 1 | 3.752954 | 3.466075 | 0.413096 | 32.010853 | 01:45 | . 2 | 3.695044 | 3.380348 | 0.420731 | 29.381001 | 01:45 | . 3 | 3.633570 | 3.311184 | 0.427943 | 27.417580 | 01:44 | . 4 | 3.567487 | 3.263233 | 0.432484 | 26.133884 | 01:43 | . 5 | 3.499947 | 3.227200 | 0.436647 | 25.208973 | 01:45 | . 6 | 3.441441 | 3.206076 | 0.438934 | 24.682047 | 01:45 | . 7 | 3.396994 | 3.195556 | 0.440499 | 24.423742 | 01:44 | . 8 | 3.362832 | 3.190961 | 0.441435 | 24.311789 | 01:40 | . 9 | 3.352602 | 3.190133 | 0.441630 | 24.291660 | 01:37 | . The accuracy above represents the models ability to predict the next word in a sequence from our disaster tweets data. 44.16%! That&#39;s pretty dang good for something that took about the same time as our Word2Vec models. . But we&#39;re not after text prediction, we&#39;re after text classification. Let&#39;s turn to that now. . First, let&#39;s create a DataBlock that we&#39;ll pass to text_classifier_learner. Notice that now we&#39;re passing two blocks to the blocks parameter: TextBlock and CategoryBlock. We specify these with the get_x and get_y parameters. It is also important to note the new TextBlock parameter vocab. Without this, the language model fitting we did above will mean nothing! . dls_clas = DataBlock( blocks=(TextBlock.from_df(&#39;text&#39;, vocab=dls_lm.vocab, seq_len=80), CategoryBlock), get_x=ColReader(&#39;text&#39;), get_y=ColReader(&#39;target&#39;), splitter=RandomSplitter() ).dataloaders(data, bs=128, seq_len=80) . Check to see that our data is how we want it. . dls_clas.show_batch(max_n=3) . text category . 0 xxbos _ xxunk xxrep 5 ? xxup retweet xxunk xxrep 7 ? xxunk xxrep 5 ? xxup follow xxup all xxup who xxup rt xxunk xxrep 7 ? xxunk xxrep 5 ? xxup xxunk xxunk xxrep 7 ? xxunk xxrep 5 ? xxup gain xxup with xxunk xxrep 7 ? xxunk xxrep 5 ? xxup follow ? xxunk # xxup xxunk xxunk # xxup ty | 0 | . 1 xxbos . : . : . : . : . : . : . : . : . : . : . : . : . : . : . : . : . : . : . : . : . : xxup rt xxunk : # xxunk n n xxmaj indian xxmaj army xxunk _ http : / / t.co / xxunk g xxpad | 0 | . 2 xxbos xxup info xxup s. xxup xxunk : xxunk / 6 . xxup xxunk : xxup xxunk xxup xxunk . xxup exp xxup xxunk xxup xxunk . xxup xxunk 05 . xxup curfew xxup in xxup xxunk xxup until xxunk xxup xxunk xxup xxunk xxup foxtrot 5 &amp; &amp; xxup foxtrot 6 xxup xxunk . xxup xxunk : 10 . xxpad xxpad xxpad xxpad xxpad | 0 | . Now it&#39;s time to create our text classifier model, again using transfer learning from the AWD_LSTM model provided by fastai. This time we want to see the accuracy and F1 score when testing on the validation set. . learn = text_classifier_learner( dls_clas, AWD_LSTM, drop_mult=0.5, metrics=[accuracy, F1Score()]) . Now we can fit: . learn.fit_one_cycle(1, 2e-2) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.752347 | 0.520742 | 0.745729 | 0.674516 | 00:43 | . And that&#39;s. It. . Crazy, right?! One last step that we need to take care of to inch our models accuracy up further is gradual unfreezing. Unfreezing a few layers at a time seems to make a meaningful difference in NLP, so we&#39;ll do that here (in computer vision, the model will often be unfrozen all at once). . learn.freeze_to(-2) learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2)) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.643792 | 0.498137 | 0.766754 | 0.712084 | 00:52 | . learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3)) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.547749 | 0.476135 | 0.777267 | 0.709512 | 01:32 | . learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3)) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.491142 | 0.465297 | 0.783180 | 0.732143 | 02:07 | . 1 | 0.473932 | 0.457790 | 0.793036 | 0.737719 | 02:08 | . After fully unfreezing and fitting our model, our accuracy is... 79.30%! Over 11% better than our best Word2Vec! Impressive. Our F1 score of 0.738 also blows away our best Word2Vec F1 score of 0.433. Impressive, indeed. . But how will transfer learning perform will the preprocessed tweets? Let&#39;s find out! . Transfer Learning with &quot;Simple&quot; Tweets . In order to repeat the same process for transfer learning on the preprocessed tweets, we&#39;ll need to create a whole new language model for each set. This is done almost exactly in the same way as above. The two differences are: . The column that your selecting from will change from text to text_simple or text_spacy. | The get_x parameter when creating the DataBlock for the text_classifier_learner, dls_clas, must remain text, no matter the name of the column in the DataFrame that you are using as the independent variable. [1] | Knowing this, let&#39;s fit our language model! . dls_lm = DataBlock( blocks=(TextBlock.from_df(&#39;text_simple&#39;, is_lm=True)), get_items=ColReader(&#39;text_simple&#39;), splitter=RandomSplitter(0.1) ).dataloaders(data, bs=128, seq_len=80) learn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]) learn.fit_one_cycle(1, 2e-2) learn.fit_one_cycle(10,2e-3) dls_clas = DataBlock( blocks=(TextBlock.from_df(&#39;text_simple&#39;, vocab=dls_lm.vocab, seq_len=80), CategoryBlock), get_x=ColReader(&#39;text&#39;), get_y=ColReader(&#39;target&#39;), splitter=RandomSplitter() ).dataloaders(data, bs=128, seq_len=80) . . epoch train_loss valid_loss accuracy perplexity time . 0 | 6.009004 | 5.068555 | 0.197061 | 158.944473 | 00:58 | . epoch train_loss valid_loss accuracy perplexity time . 0 | 5.313193 | 5.048050 | 0.201638 | 155.718506 | 00:57 | . 1 | 5.282051 | 4.972396 | 0.211692 | 144.372375 | 00:57 | . 2 | 5.232960 | 4.881814 | 0.219846 | 131.869598 | 00:58 | . 3 | 5.169006 | 4.806246 | 0.229412 | 122.271782 | 00:57 | . 4 | 5.102448 | 4.749909 | 0.234766 | 115.573761 | 00:57 | . 5 | 5.040435 | 4.709253 | 0.237660 | 110.969269 | 01:01 | . 6 | 4.987535 | 4.681636 | 0.241375 | 107.946518 | 01:02 | . 7 | 4.935099 | 4.666973 | 0.243062 | 106.375267 | 01:02 | . 8 | 4.900142 | 4.660895 | 0.244003 | 105.730652 | 01:02 | . 9 | 4.867511 | 4.659934 | 0.244407 | 105.629066 | 01:02 | . Fit our text classifier: . learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=[accuracy, F1Score()]) learn.fit_one_cycle(1, 2e-2) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.733424 | 0.527626 | 0.749014 | 0.694400 | 00:27 | . Now gradually unfreeze: . learn.freeze_to(-2) learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2)) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.613524 | 0.516843 | 0.743101 | 0.703113 | 00:33 | . learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3)) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.536539 | 0.490117 | 0.762155 | 0.720247 | 00:56 | . learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3)) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.477936 | 0.467683 | 0.787122 | 0.732673 | 01:19 | . 1 | 0.460449 | 0.468069 | 0.785808 | 0.731023 | 01:20 | . Accuracy: 78.58%. F1 score: 0.731. . Nearly the same as, but not quite better than the unprocessed tweets. This is the opposite of what happened with TF-IDF and Word2Vec. . Let&#39;s see how the SpaCy tweets perform: . Transfer Learning with SpaCy Tweets . Let&#39;s do the same thing with our tweets preprocessed with SpaCy. . First, the language model: . dls_lm = DataBlock( blocks=(TextBlock.from_df(&#39;text_spacy&#39;, is_lm=True)), get_items=ColReader(&#39;text_spacy&#39;), splitter=RandomSplitter(0.1) ).dataloaders(data, bs=128, seq_len=80) learn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]) learn.fit_one_cycle(1, 2e-2) learn.fit_one_cycle(10,2e-3) dls_clas = DataBlock( blocks=(TextBlock.from_df(&#39;text_spacy&#39;, vocab=dls_lm.vocab, seq_len=80), CategoryBlock), get_x=ColReader(&#39;text&#39;), get_y=ColReader(&#39;target&#39;), splitter=RandomSplitter() ).dataloaders(data, bs=128, seq_len=80) . . epoch train_loss valid_loss accuracy perplexity time . 0 | 5.682709 | 4.324547 | 0.359819 | 75.531273 | 01:02 | . epoch train_loss valid_loss accuracy perplexity time . 0 | 4.665442 | 4.285821 | 0.361470 | 72.662209 | 01:01 | . 1 | 4.583601 | 4.133880 | 0.388832 | 62.419621 | 01:01 | . 2 | 4.486686 | 3.995918 | 0.411705 | 54.375725 | 01:02 | . 3 | 4.398295 | 3.909034 | 0.419096 | 49.850784 | 01:01 | . 4 | 4.326467 | 3.845958 | 0.424627 | 46.803497 | 01:02 | . 5 | 4.241285 | 3.802249 | 0.428516 | 44.801819 | 01:01 | . 6 | 4.178056 | 3.773062 | 0.433052 | 43.513077 | 01:01 | . 7 | 4.115740 | 3.756646 | 0.434881 | 42.804615 | 01:01 | . 8 | 4.073174 | 3.751379 | 0.435884 | 42.579758 | 01:01 | . 9 | 4.039183 | 3.750412 | 0.436159 | 42.538593 | 01:01 | . Then, fit the text classifier: . learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=[accuracy, F1Score()]) learn.fit_one_cycle(1, 2e-2) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.796044 | 0.537777 | 0.747043 | 0.698039 | 00:30 | . Gradually unfreeze the model: . learn.freeze_to(-2) learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2)) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.683284 | 0.511340 | 0.770039 | 0.720893 | 00:37 | . learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3)) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.595137 | 0.488891 | 0.781209 | 0.722269 | 01:02 | . learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3)) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.554800 | 0.465724 | 0.790407 | 0.739592 | 01:29 | . 1 | 0.537899 | 0.456067 | 0.789750 | 0.724138 | 01:28 | . Et voilà! . Accuracy: 79.00%. F1 score: 0.724. . So between the three datasets used in transfer learning, the unprocessed dataset seemed to perform the best! Unexpected, indeed. . Conclusion . Now that we&#39;ve gone through each model: TF-IDF, Word2Vec, and transfer learning, it&#39;s time to compare the results: . Model Dataset Accuracy F1 Score . TF-IDF | Unprocessed | 64.08% | 0.530 | . &#39;&#39; | &quot;Simple&quot; | 66.60% | 0.570 | . &#39;&#39; | SpaCy | 64.02% | 0.489 | . Word2Vec | Unprocessed | 63.39% | 0.278 | . &#39;&#39; | &quot;Simple&quot; | 67.28% | 0.433 | . &#39;&#39; | SpaCy | 64.44% | 0.324 | . Transfer Learning | Unprocessed | 79.30% | 0.738 | . &#39;&#39; | &quot;Simple&quot; | 75.58% | 0.731 | . &#39;&#39; | SpaCy | 79.00% | 0.724 | . And the winner is, unsurprisingly, transfer learning! What is surprising, however, is that of the three datasets that we used for transfer learning, the unprocessed dataset yielded the best results. This provides strong support for transfer learning, as it is able to extract nuances in natural language as opposed to augemented, unnatural language. . If you&#39;re interesed in getting more involved with transfer learning, I strongly recommend Jeremy Howard and Rachel Thomas&#39; course Deep Learning for Coders. At the time of writing, this is an excellent resource for getting a really good, modern grasp of deep learning, provided you&#39;ve got some basic Python programming experience. And it&#39;s all free! . With that, I&#39;ll leave the reader to experiment further with text classification and langauge modeling. . Questions I&#39;m now asking myself: . What other preprocessing methods or data augmentations techniques could we have used? | What&#39;s a transformer? | How does BERT work? | Where else can we apply text classification to somehow learn something meaningful? | .",
            "url": "https://davidbyron.info/blog/tf-idf/word2vec/neural%20networks/text%20classification/natural%20language%20processing/2020/10/31/Text-Classification-Comparison.html",
            "relUrl": "/tf-idf/word2vec/neural%20networks/text%20classification/natural%20language%20processing/2020/10/31/Text-Classification-Comparison.html",
            "date": " • Oct 31, 2020"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://davidbyron.info/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}